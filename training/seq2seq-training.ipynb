{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11208224,"sourceType":"datasetVersion","datasetId":6998547}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Import libraries and Setup","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import T5Tokenizer\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T17:09:29.761345Z","iopub.execute_input":"2025-03-29T17:09:29.761637Z","iopub.status.idle":"2025-03-29T17:09:29.787690Z","shell.execute_reply.started":"2025-03-29T17:09:29.761613Z","shell.execute_reply":"2025-03-29T17:09:29.786852Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### Load dataset","metadata":{}},{"cell_type":"code","source":"# Load the TSV file\nfile_path = \"/kaggle/input/paradetox/paradetox.tsv\"\ndf = pd.read_csv(file_path, sep=\"\\t\", encoding=\"utf-8\")\n\n# Display the first few rows to verify\nprint(df.head())\nprint(f\"Dataset size: {len(df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T17:09:29.788791Z","iopub.execute_input":"2025-03-29T17:09:29.789055Z","iopub.status.idle":"2025-03-29T17:09:29.905666Z","shell.execute_reply.started":"2025-03-29T17:09:29.789034Z","shell.execute_reply":"2025-03-29T17:09:29.904972Z"}},"outputs":[{"name":"stdout","text":"                                               toxic  \\\n0                           he had steel balls too !   \n1  dude should have been taken to api , he would ...   \n2  im not gonna sell the fucking picture , i just...   \n3  the garbage that is being created by cnn and o...   \n4  the reason they dont exist is because neither ...   \n\n                                            neutral1  \\\n0                                  he was brave too!   \n1  It would have been good if he went to api. He ...   \n2  I'm not gonna sell the picture, i just want to...   \n3  the news that is being created by cnn and othe...   \n4  The reason they don't exist is because neither...   \n\n                                            neutral2  \\\n0                                                NaN   \n1                                                NaN   \n2                                                NaN   \n3  The news that is being created by cnn and othe...   \n4                                                NaN   \n\n                                            neutral3  \n0                                                NaN  \n1                                                NaN  \n2                                                NaN  \n3  the garbage that is being created by cnn and o...  \n4                                                NaN  \nDataset size: 11927\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### Data merging and processing","metadata":{}},{"cell_type":"code","source":"# Melt the DataFrame to pair each toxic sentence with all neutral paraphrases\ndf_melted = pd.melt(\n    df,\n    id_vars=[\"toxic\"],\n    value_vars=[\"neutral1\", \"neutral2\", \"neutral3\"],\n    var_name=\"neutral_type\",\n    value_name=\"target_text\"\n)\n\n# Remove rows where target_text is NaN or empty\ndf_melted = df_melted[\n    df_melted[\"target_text\"].notna() & (df_melted[\"target_text\"].str.strip() != \"\")\n].reset_index(drop=True)\n\n# Add task prefix to toxic sentences\nprefix = \"detoxify: \"\ndf_melted[\"input_text\"] = prefix + df_melted[\"toxic\"]\n\n# Drop unnecessary column and rename\ndf_melted = df_melted[[\"input_text\", \"target_text\"]]\n\nprint(df_melted.head(8))\nprint(f\"Expanded dataset size: {len(df_melted)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T17:09:29.907401Z","iopub.execute_input":"2025-03-29T17:09:29.907631Z","iopub.status.idle":"2025-03-29T17:09:29.952387Z","shell.execute_reply.started":"2025-03-29T17:09:29.907612Z","shell.execute_reply":"2025-03-29T17:09:29.951494Z"}},"outputs":[{"name":"stdout","text":"                                          input_text  \\\n0                 detoxify: he had steel balls too !   \n1  detoxify: dude should have been taken to api ,...   \n2  detoxify: im not gonna sell the fucking pictur...   \n3  detoxify: the garbage that is being created by...   \n4  detoxify: the reason they dont exist is becaus...   \n5      detoxify: i hope they beat each other silly .   \n6  detoxify: no good bastards that we are and we ...   \n7       detoxify: stop the coverage and let em rot .   \n\n                                         target_text  \n0                                  he was brave too!  \n1  It would have been good if he went to api. He ...  \n2  I'm not gonna sell the picture, i just want to...  \n3  the news that is being created by cnn and othe...  \n4  The reason they don't exist is because neither...  \n5                       I think they beat each other  \n6  no good people that we are and we are unrepent...  \n7                           kindly stop the coverage  \nExpanded dataset size: 19743\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"We can see that some detoxified data has the same sentence as the toxic input, which isn't ideal. We will do a deeper filtering.","metadata":{}},{"cell_type":"code","source":"# Additional filter to remove identical pairs\ndf_melted = df_melted[df_melted[\"input_text\"] != prefix + df_melted[\"target_text\"]].reset_index(drop=True)\nprint(f\"Filtered dataset size: {len(df_melted)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T17:09:29.953540Z","iopub.execute_input":"2025-03-29T17:09:29.953818Z","iopub.status.idle":"2025-03-29T17:09:29.965747Z","shell.execute_reply.started":"2025-03-29T17:09:29.953797Z","shell.execute_reply":"2025-03-29T17:09:29.964981Z"}},"outputs":[{"name":"stdout","text":"Filtered dataset size: 19617\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### Load tokenizer","metadata":{}},{"cell_type":"code","source":"# Load T5 tokenizer\ntokenizer = T5Tokenizer.from_pretrained(\"t5-base\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T17:09:29.966665Z","iopub.execute_input":"2025-03-29T17:09:29.966981Z","iopub.status.idle":"2025-03-29T17:09:30.723682Z","shell.execute_reply.started":"2025-03-29T17:09:29.966949Z","shell.execute_reply":"2025-03-29T17:09:30.723010Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"308c633fc049420eaae1c8fd9fb3e728"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e95fbecef883472e978f6d4accb17c5a"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Tokenization function\ndef tokenize_batch(texts, max_length=128):\n    encodings = tokenizer(\n        texts.tolist(),\n        max_length=max_length,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\"\n    )\n    return encodings[\"input_ids\"], encodings[\"attention_mask\"]\n\n# Tokenize inputs and targets\ninput_ids, input_masks = tokenize_batch(df_melted[\"input_text\"])\ntarget_ids, target_masks = tokenize_batch(df_melted[\"target_text\"])\n\n# Shift target_ids for teacher forcing\ntarget_ids_shifted = torch.cat([torch.full((target_ids.size(0), 1), tokenizer.pad_token_id), target_ids[:, :-1]], dim=1)\n\nprint(f\"Input IDs shape: {input_ids.shape}\")\nprint(f\"Target IDs shape: {target_ids.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T17:09:30.724363Z","iopub.execute_input":"2025-03-29T17:09:30.724574Z","iopub.status.idle":"2025-03-29T17:09:36.740047Z","shell.execute_reply.started":"2025-03-29T17:09:30.724555Z","shell.execute_reply":"2025-03-29T17:09:36.739174Z"}},"outputs":[{"name":"stdout","text":"Input IDs shape: torch.Size([19617, 128])\nTarget IDs shape: torch.Size([19617, 128])\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### Dataset Split","metadata":{}},{"cell_type":"code","source":"# Split indices\ntrain_idx, temp_idx = train_test_split(range(len(df_melted)), test_size=0.2, random_state=42)\nval_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n\n# Create splits\ntrain_data = {\n    \"input_ids\": input_ids[train_idx],\n    \"input_masks\": input_masks[train_idx],\n    \"target_ids\": target_ids[train_idx],\n    \"target_masks\": target_masks[train_idx],\n    \"target_ids_shifted\": target_ids_shifted[train_idx]\n}\n\nval_data = {\n    \"input_ids\": input_ids[val_idx],\n    \"input_masks\": input_masks[val_idx],\n    \"target_ids\": target_ids[val_idx],\n    \"target_masks\": target_masks[val_idx],\n    \"target_ids_shifted\": target_ids_shifted[val_idx]\n}\n\ntest_data = {\n    \"input_ids\": input_ids[test_idx],\n    \"input_masks\": input_masks[test_idx],\n    \"target_ids\": target_ids[test_idx],\n    \"target_masks\": target_masks[test_idx],\n    \"target_ids_shifted\": target_ids_shifted[test_idx]\n}\n\nprint(f\"Train size: {len(train_idx)}, Val size: {len(val_idx)}, Test size: {len(test_idx)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T17:09:36.740967Z","iopub.execute_input":"2025-03-29T17:09:36.741341Z","iopub.status.idle":"2025-03-29T17:09:36.828633Z","shell.execute_reply.started":"2025-03-29T17:09:36.741302Z","shell.execute_reply":"2025-03-29T17:09:36.827726Z"}},"outputs":[{"name":"stdout","text":"Train size: 15693, Val size: 1962, Test size: 1962\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### PyTorch Dataset and DataLoader","metadata":{}},{"cell_type":"code","source":"class DetoxDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n    \n    def __len__(self):\n        return len(self.data[\"input_ids\"])\n    \n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.data[\"input_ids\"][idx],\n            \"attention_mask\": self.data[\"input_masks\"][idx],\n            \"labels\": self.data[\"target_ids_shifted\"][idx],\n            \"decoder_attention_mask\": self.data[\"target_masks\"][idx]\n        }\n\n# Instantiate datasets\ntrain_dataset = DetoxDataset(train_data)\nval_dataset = DetoxDataset(val_data)\ntest_dataset = DetoxDataset(test_data)\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# Test a batch\nbatch = next(iter(train_loader))\nprint(\"Sample batch:\", {k: v.shape for k, v in batch.items()})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T17:09:36.830516Z","iopub.execute_input":"2025-03-29T17:09:36.830766Z","iopub.status.idle":"2025-03-29T17:09:36.849799Z","shell.execute_reply.started":"2025-03-29T17:09:36.830745Z","shell.execute_reply":"2025-03-29T17:09:36.848952Z"}},"outputs":[{"name":"stdout","text":"Sample batch: {'input_ids': torch.Size([16, 128]), 'attention_mask': torch.Size([16, 128]), 'labels': torch.Size([16, 128]), 'decoder_attention_mask': torch.Size([16, 128])}\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### Verification","metadata":{}},{"cell_type":"code","source":"# Filter original DataFrame for a row with multiple neutrals\nsample_row = df[df[\"neutral2\"].notna()].iloc[0]\nprint(\"Original toxic:\", sample_row[\"toxic\"])\nprint(\"Neutral1:\", sample_row[\"neutral1\"])\nprint(\"Neutral2:\", sample_row[\"neutral2\"])\nprint(\"Neutral3:\", sample_row[\"neutral3\"])\n\n# Check corresponding rows in melted DataFrame\nprint(\"\\nExpanded pairs:\")\nprint(df_melted[df_melted[\"input_text\"] == prefix + sample_row[\"toxic\"]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T17:09:36.850986Z","iopub.execute_input":"2025-03-29T17:09:36.851409Z","iopub.status.idle":"2025-03-29T17:09:36.865915Z","shell.execute_reply.started":"2025-03-29T17:09:36.851381Z","shell.execute_reply":"2025-03-29T17:09:36.865272Z"}},"outputs":[{"name":"stdout","text":"Original toxic: the garbage that is being created by cnn and other news agencies is outrageous .\nNeutral1: the news that is being created by cnn and other news agencies is outrageous.\nNeutral2: The news that is being created by cnn and other news agencies is outrageous.\nNeutral3: the garbage that is being created by cnn and other news agencies is outrageous .\n\nExpanded pairs:\n                                              input_text  \\\n3      detoxify: the garbage that is being created by...   \n11830  detoxify: the garbage that is being created by...   \n\n                                             target_text  \n3      the news that is being created by cnn and othe...  \n11830  The news that is being created by cnn and othe...  \n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"The sentence that's identical with the toxic one has been filtered out.","metadata":{}},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration, AdamW\n\n# Load T5 model\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=3e-5)\n\n# Training loop\nnum_epochs = 3\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for batch in train_loader:\n        optimizer.zero_grad()\n        \n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        decoder_attention_mask = batch[\"decoder_attention_mask\"].to(device)\n        \n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels,\n            decoder_attention_mask=decoder_attention_mask\n        )\n        \n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n    \n    # Validation\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            decoder_attention_mask = batch[\"decoder_attention_mask\"].to(device)\n            \n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels,\n                decoder_attention_mask=decoder_attention_mask\n            )\n            val_loss += outputs.loss.item()\n    \n    avg_val_loss = val_loss / len(val_loader)\n    print(f\"Validation Loss: {avg_val_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T17:09:36.866666Z","iopub.execute_input":"2025-03-29T17:09:36.866878Z","iopub.status.idle":"2025-03-29T17:56:41.373432Z","shell.execute_reply.started":"2025-03-29T17:09:36.866860Z","shell.execute_reply":"2025-03-29T17:56:41.372641Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3, Average Loss: 0.3629\nValidation Loss: 0.1289\nEpoch 2/3, Average Loss: 0.1364\nValidation Loss: 0.1110\nEpoch 3/3, Average Loss: 0.1200\nValidation Loss: 0.1044\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### Test","metadata":{}},{"cell_type":"code","source":"model.eval()\ntest_sentence = \"detoxify: You're a fucking moron!\"\ninputs = tokenizer(test_sentence, return_tensors=\"pt\", max_length=128, \n                   padding=\"max_length\", truncation=True)\ninput_ids = inputs[\"input_ids\"].to(device)\nattention_mask = inputs[\"attention_mask\"].to(device)\n\n# Generate detoxified output\noutputs = model.generate(\n    input_ids=input_ids,\n    attention_mask=attention_mask,\n    max_length=128,\n    num_beams=4,\n    early_stopping=True\n)\n\ndetoxified = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(f\"Toxic: {test_sentence}\")\nprint(f\"Detoxified: {detoxified}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T17:58:27.591001Z","iopub.execute_input":"2025-03-29T17:58:27.591386Z","iopub.status.idle":"2025-03-29T17:58:29.026570Z","shell.execute_reply.started":"2025-03-29T17:58:27.591360Z","shell.execute_reply":"2025-03-29T17:58:29.025841Z"}},"outputs":[{"name":"stdout","text":"Toxic: detoxify: You're a fucking moron!\nDetoxified: You're a bad person!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T17:59:17.211707Z","iopub.execute_input":"2025-03-29T17:59:17.211990Z","iopub.status.idle":"2025-03-29T17:59:19.123810Z","shell.execute_reply.started":"2025-03-29T17:59:17.211970Z","shell.execute_reply":"2025-03-29T17:59:19.123098Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}